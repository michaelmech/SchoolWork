{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will perform a binary classification task on a modified Iris dataset. This modified Iris dataset has 2 classes and it is split into a training set $S_\\text{training}$ with 100 data points and a test set $S_\\text{test}$ with 50 data points. Each data point $(\\mathbf{x},y)$ has a feature vector $\\mathbf{x} \\in \\mathbb{R}^4$ and its corresponding label $y \\in \\{0, 1\\}$.\n",
    "\n",
    "(In fact, the original Iris dataset has 3 classes: 0 for Setosa, 1 for Versicolor and 2 for Virginica. Here for binary classification task, we combine Setosa and Versicolor together as $y = 0$ and label Virginca as $y = 1$)\n",
    "\n",
    "Here we utilize a decision stump to solve the above binary classification task. The decision stump works as follows (for simplicity, we restrict our attention to uni-directional decision stumps):\n",
    "\n",
    "- Given the feature vector $\\mathbf{x}$, the feature index $j$, and a threshold $Th$, the classification function is defined by $y=f(\\mathbf{x}, j, Th)$ as:\n",
    "$$\n",
    "f({\\bf x}, j, Th)=\n",
    "\\begin{cases}\n",
    "1 & if \\; {\\bf x}(j) \\geq Th \\\\\n",
    "0 & otherwise.\n",
    "\\end{cases} \n",
    "$$\n",
    "where $\\mathbf{x}(j)$ refers to the $j$-th feature in $\\mathbf{x}$.\n",
    "- The error $e$ on dataset $S=\\{(\\mathbf{x}_i,y_i)\\}$ is defined as:\n",
    "    $$e = \\frac{1}{n}\\sum_{i=1}^n\\mathbf{1}\\big(y_i \\neq f(\\mathbf{x}_i,j,Th)\\big)$$\n",
    "    where $n=|S|$ is the size of the dataset $S$. Thus, we can obtain training error $e_\\text{training}$ on training set $S_\\text{training}$, and test error $e_\\text{test}$ on test set $S_\\text{test}$.\n",
    "    \n",
    "Based on the decision stump above, we wish to use an algorithm to find the \n",
    "**best feature index $j^*$** and **best threshold $Th^*$** on training set to create a \"best\" decision stump, in a sense that such decision stump can achieve the **lowest training error** $e_\\text{training}^*$.\n",
    "\n",
    "Algorithm:\n",
    "- Initialize $e_\\text{training}^*=1$\n",
    "- for $j$ = 0, 1, 2, 3:\n",
    "   - for threshold $Th$ = min($\\mathbf{x}(j)$), ..., max($\\mathbf{x}(j)$):\n",
    "       - Calculate $e_\\text{training}$\n",
    "       - If $e_\\text{training}$ < $e_\\text{training}^*$, then:\n",
    "           - $j*=j$\n",
    "           - $Th^*=Th$\n",
    "           - $e_\\text{training}^*=e_\\text{training}$\n",
    "\n",
    "- Output best feature index $j^*$, best threshold $Th^*$, lowest training error $e_\\text{training}^*$ and calculate its corresponding test error $e_\\text{test}^*$.\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T06:47:24.203437Z",
     "start_time": "2020-01-14T06:47:23.636611Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T08:54:37.409499Z",
     "start_time": "2020-01-14T08:54:37.400692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups: \n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups. \n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).reshape(-1,1) # The shape of Y is (150, 1), which means \n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value. \n",
    "\n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])               # The result should be always: [ 5.8  4.   1.2  0.2  0. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T08:54:38.535037Z",
     "start_time": "2020-01-14T08:54:38.524856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:4]\n",
    "Y_shuffled = X_and_Y[:,4]\n",
    "\n",
    "X_train = X_shuffled[:100] # Shape: (100,4)\n",
    "Y_train = Y_shuffled[:100] # Shape: (100,)\n",
    "X_test = X_shuffled[100:]  # Shape: (50,4)\n",
    "Y_test = Y_shuffled[100:]  # Shape: (50,)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best feature and best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T09:23:07.141637Z",
     "start_time": "2020-01-14T09:23:07.078455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Judge function: 1(a != b).\n",
    "def judge(a, b):\n",
    "    # This function should return 0 or 1.\n",
    "    \n",
    "    ############ To be filled. ############\n",
    "    \n",
    "    \n",
    "# Decision stump classifier: f(x, j, Th).\n",
    "def f_decision_stump(x, j, Th):\n",
    "    # x should be a 4-dimensional vector.\n",
    "    # This function should return 0 or 1.\n",
    "    \n",
    "    ############ To be filled. ############\n",
    "    \n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, j, Th):\n",
    "    ############ To be filled. ############\n",
    "    \n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        ############ To be filled. ############\n",
    "        # Hint: Use judge() and f_decision_stump()\n",
    "    \n",
    "    return ############ To be filled. ############\n",
    "    \n",
    "    \n",
    "# Main algorithm.\n",
    "opt_e_training = 1.0\n",
    "opt_j = -1\n",
    "opt_Th = -1\n",
    "for j in [0,1,2,3]:\n",
    "    for Th in np.arange(X_train[:, j].min(), X_train[:, j].max(), 0.05):\n",
    "        e_training = calc_error(X_train, Y_train, j, Th)\n",
    "        if e_training < opt_e_training:\n",
    "            ############ To be filled. ############\n",
    "            \n",
    "            \n",
    "print('Optimal: j*={}, Th*={:.2f}, e_training*={:.3f}, e_test*={:.3f}'.format(\n",
    "      opt_j, opt_Th, opt_e_training, \n",
    "      calc_error(X_test, Y_test, opt_j, opt_Th)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T09:27:23.545019Z",
     "start_time": "2020-01-14T09:27:22.857017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show the histograms of each feature and draw a line for best decision stump.\n",
    "plt.figure(figsize=(12,8))\n",
    "for j in range(4):\n",
    "    Xj_train = X_train[:,j]\n",
    "    Xj_when_Y0_train = [Xj_train[i] for i in range(len(Xj_train)) if Y_train[i] == 0]\n",
    "    Xj_when_Y1_train = [Xj_train[i] for i in range(len(Xj_train)) if Y_train[i] == 1]\n",
    "\n",
    "    plt.subplot(2, 2, j+1)\n",
    "    plt.hist(Xj_when_Y0_train, label='Feature {}, Y=0'.format(j))\n",
    "    plt.hist(Xj_when_Y1_train, label='Feature {}, Y=1'.format(j))\n",
    "    plt.xlabel('Feature {}: {}'.format(j, iris.feature_names[j]))\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    if j == opt_j:\n",
    "        plt.plot([opt_Th, opt_Th], [0, 10])\n",
    "        plt.text(opt_Th, 10, 'e_training*: {}'.format(opt_e_training))\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
